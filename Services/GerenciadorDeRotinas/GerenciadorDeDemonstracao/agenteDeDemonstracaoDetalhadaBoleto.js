// üîÑ Agente GPT completo com l√≥gica igual a identificarModeloPorNome
const { sendBotMessage } = require("../../messageSender");
const {
  setUserStage,
  appendToConversation,
  getConversation,
  getNomeUsuario,
  getUserStage,
} = require("../../redisService");
const { getAllCelulareBoleto } = require("../../dbService");
const { rotinaDeAgendamento } = require("../../GerenciadorDeRotinas/GerenciadorDeAgendamento/rotinaDeAgendamento");
const OpenAI = require("openai");
require("dotenv").config();
const { informacoesPayjoy } = require("../../utils/informacoesPayjoy");
const { gatilhosEmocionaisVertex } = require('../../utils/gatilhosEmocionais');
const { tomDeVozVertex } = require('../../utils/tomDeVozVertex');
const { obje√ß√µesVertexBoleto } = require("../../utils/objecoesBoleto");;
const { tomDeVozVertexData } = require("../../../Services/utils/tomDeVozVertexData");
const { extrairTextoDoQuotedMessage } = require("../../utils/utilitariosDeMensagem/extrairTextoDoQuotedMessage");
const { sanitizarEntradaComQuoted } = require("../../utils/utilitariosDeMensagem/sanitizarEntradaComQuoted");
const { prepararContextoDeModelosRecentes } = require("../../utils/utilitariosDeMensagem/prepararContextoDeModelosRecentes");

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

const obterModelosDoBling = async () => {
  const celulares = await getAllCelulareBoleto();

  const termosIgnorados = [
    "BLACK", "WHITE", "BLUE", "GREEN", "GOLD", "PURPLE", "SILVER", "CORAL",
    "MIDNIGHT", "OCEAN", "TEAL", "AZUL", "VERDE", "LAVENDER", "VOYAGE",
    "MARBLE", "STORM", "LIGHTNING", "SPARKLE", "DARK", "LIME", "STAR", "STARRY",
    "OC√âANO", "ROM", "RAM"
  ];

  const normalizeNome = (nome) =>
    nome
      .replace(/^smartphone\s*/i, "")
      .replace(/[^\w\s]/gi, "")
      .trim()
      .split(/\s+/)
      .filter((p) => !termosIgnorados.includes(p.toUpperCase()))
      .join(" ")
      .toLowerCase()
      .trim();

  const mapaUnico = new Map();

  for (const c of celulares) {
    const chave = normalizeNome(c.nome);
    if (!mapaUnico.has(chave)) {
      mapaUnico.set(chave, {
        nome: c.nome,
        preco: c.preco,
        descricaoCurta: c.descricao, // ‚úÖ Corrigido aqui!
        imagemURL: c.imageURL,
        precoParcelado: c.precoParcelado,
        fraseImpacto: c.fraseImpacto,
        subTitulo: c.subTitulo,
        videoURL: c.videoURL,
      });
    }
  }

  return Array.from(mapaUnico.values());
};

const calcularSimilaridadePorEmbeddings = async (entrada, modelos) => {
  const entradaEmbedding = await openai.embeddings.create({ model: "text-embedding-3-small", input: entrada });
  const modelosEmbedding = await openai.embeddings.create({ model: "text-embedding-3-small", input: modelos.map(m => m.nome) });
  const vetorEntrada = entradaEmbedding.data[0].embedding;
  return modelosEmbedding.data.map((item, i) => {
    const m = modelos[i];
    const score = vetorEntrada.reduce((acc, val, idx) => acc + val * item.embedding[idx], 0);
    return { ...m, score };
  }).sort((a, b) => b.score - a.score);
}; 

const agenteDeDemonstracaoDetalhadaBoleto = async ({ sender, msgContent, pushName }) => {
  try {
    await setUserStage(sender, "agente_de_demonstra√ß√£o_detalhada_boleto");

    const nome = await getNomeUsuario(sender);
    const textoQuoted = extrairTextoDoQuotedMessage(msgContent);

    let entradaAtual = typeof msgContent === "string" ? msgContent : msgContent?.termosRelacionados || "";     

    await appendToConversation(sender, {
      tipo: "entrada_usuario",
      conteudo: entradaAtual,
      timestamp: new Date().toISOString()
    });

    const conversaArray = await getConversation(sender);
    const conversaCompleta = conversaArray
      .map(msg => {
        try {
          const json = typeof msg === "string" ? JSON.parse(msg) : msg;
          return json.conteudo || "";
        } catch {
          return typeof msg === "string" ? msg : "";
        }
      })
      .filter(Boolean)
      .slice(-10)
      .join(" | ");

    const listaModelos = await obterModelosDoBling();

    const similares = await calcularSimilaridadePorEmbeddings(entradaAtual, listaModelos);
    console.log(`Esse foi o match de similaridade${similares}`)
    const maisProvavel = similares?.[0];
    console.log(`Esse foi o match de similaridade${maisProvavel}`)

    if (maisProvavel?.score > 0.90) {
      console.log("‚úÖ Entrada casa fortemente com modelo:", maisProvavel.modelo);
      await appendToConversation(sender, {
        tipo: "deliberacao_toa",
        conteudo: {
          acao: "mostrarResumoModeloBoleto",
          motivo: `Cliente mencionou ${maisProvavel.modelo} com alta similaridade`,
          argumento: { modeloMencionado: maisProvavel.modelo }
        },
        timestamp: new Date().toISOString()
      });

      return await mostrarResumoModeloBoleto(sender, {
        modeloMencionado: maisProvavel.modelo
      }, { msgContent: entradaAtual });
    }

    const promptTOA = `
ü§ñ Voc√™ √© Anna, assistente virtual da Vertex Store.

Seu objetivo √© identificar a a√ß√£o ideal com base na inten√ß√£o do cliente.

üìå Entrada:
"${entradaAtual}"

üìú Hist√≥rico da conversa:
${conversaCompleta}

üì¶ Modelos dispon√≠veis:
${listaModelos.map(m => `- ${m.nome}`).join("\n")}

1. Se ‚Äî e SOMENTE SE ‚Äî o cliente disser explicitamente frases como "fechou", "quero esse", "vamos fechar", "√© esse mesmo", "bora", "fechado", ou mencionar uma data exata de fechamento como "vou hoje", "passo a√≠ amanh√£", "m√™s que vem", ent√£o ele est√° confirmando um dos modelos sugeridos. Escolha "fecharVenda".

‚ö†Ô∏è S√≥ escolha "fecharVenda" se j√° houver uma execu√ß√£o anterior da a√ß√£o "mostrarResumoModeloBoleto" com o modelo confirmado. Passe o modelo com argumento

2. Se o cliente fizer **qualquer pergunta**, mesmo curta (ex.: "√© bom?", "qual o pre√ßo?", "tem c√¢mera boa?"), isso significa que ele ainda est√° com d√∫vida e precisa de mais informa√ß√µes. Escolha "responderDuvida".

‚ö†Ô∏è SE o cliente mencionar claramente um modelo (ex: "o note 60 √© bom?"), voc√™ DEVE preencher o campo "argumento.nomeModelo" com o nome exato do modelo mencionado.

3. Se ele mencionar um modelo da lista e n√£o for uma pergunta, escolha "mostrarResumoModeloBoleto".

‚ö†Ô∏è Quando escolher "mostrarResumoModeloBoleto" voc√™ tamb√©m DEVE preencher "argumento.nomeModelo" com o nome exato do modelo citado.



Retorne apenas isso:
{
  "acao": "NOME_DA_ACAO",
  "motivo": "Texto explicando por que esta a√ß√£o foi escolhida",
  "argumento": {}
}`;

    const deliberacao = await openai.chat.completions.create({
      model: "gpt-4o",
      messages: [{ role: "user", content: promptTOA }],
      temperature: 0.8
    });

    const jsonMatch = deliberacao.choices?.[0]?.message?.content?.match(/\{[\s\S]*\}/);
    if (!jsonMatch) {
      console.error("‚ùå TOA n√£o retornou JSON v√°lido.");
      return await sendBotMessage(sender, "‚ö†Ô∏è N√£o consegui entender sua escolha. Pode repetir?");
    }    

    const { acao, motivo, argumento } = JSON.parse(jsonMatch[0]);

    console.log("üß† TOA escolheu:", acao, "‚Üí", motivo);

    await appendToConversation(sender, {
      tipo: "deliberacao_toa",
      conteudo: { acao, motivo, argumento },
      timestamp: new Date().toISOString()
    });

    if (!handlers[acao]) {
      return await sendBotMessage(sender, "‚ö†Ô∏è N√£o entendi sua inten√ß√£o. Pode repetir?");
    }

    return await handlers[acao](sender, argumento, { msgContent: entradaAtual });

  } catch (error) {
    console.error("‚ùå Erro no agenteDeDemonstracaoDetalhadaBoleto:", error);
    return await sendBotMessage(sender, "‚ö†Ô∏è Ocorreu um erro. Pode tentar de novo?");
  }
};

const handlers = {
  fecharVenda: async (sender, _args, extras) => {
    const { modeloEscolhido, pushName, msgContent } = extras;
    return await rotinaDeAgendamento({ sender, msgContent, pushName });
  },
  mostrarResumoModeloBoleto: async (sender, args, extras) => {
    await setUserStage(sender, "agente_de_demonstra√ß√£o_detalhada_boleto");
  
    const nome = await getNomeUsuario(sender);
    let modelo = extras?.modeloEscolhido;
  
    const normalize = (str) =>
      str
        .toLowerCase()
        .normalize("NFD")
        .replace(/[\u0300-\u036f]/g, "")
        .replace(/[^\w\s]/gi, "")
        .replace(/\s+/g, " ")
        .trim();
  
    const listaModelos = await obterModelosDoBling();
  
    // üîé Se n√£o veio modelo direto, tenta pelo nome passado como argumento
    if (!modelo && args?.nomeModelo) {
      modelo = listaModelos.find(m => normalize(m.nome) === normalize(args.nomeModelo));
    }
  
    // üîç Fallback: tenta recuperar do hist√≥rico
    if (!modelo) {
      const historico = await getConversation(sender);
      const confirmados = historico
        .filter(m => m.tipo === "modelo_confirmado")
        .map(m => typeof m.conteudo === "string" ? m.conteudo : "")
        .filter(Boolean);
  
      // ‚ö†Ô∏è Se houver mais de um modelo confirmado, pede escolha expl√≠cita
      if (confirmados.length > 1) {
        return await sendBotMessage(sender, `üìå Perfeito, ${nome}! Conversamos sobre mais de um modelo.\nPode me dizer qual deles voc√™ quer ver agora?`);
      }
  
      // ‚úÖ Se houver apenas um, tenta localizar no cat√°logo
      if (confirmados.length === 1) {
        const confirmado = confirmados[0];
        modelo = listaModelos.find(m => normalize(m.nome) === normalize(confirmado));
      }
    }
  
    if (!modelo) {
      return await sendBotMessage(sender, `‚ö†Ô∏è Opa ${nome}, n√£o consegui identificar com clareza o modelo que voc√™ quer ver.\nPode me dizer o nome exato?`);
    }
  
    // üì¶ Prompt resumido para IA gerar o pitch do modelo
    const prompt = [
      {
        role: "system",
        content: `Voc√™ √© um vendedor persuasivo e direto. 
  Seja direto, com no m√°ximo 3 frases curtas. Priorize clareza e impacto, n√£o ultrapasse 250 caracteres no total.
  
  ***Fa√ßa o mais resumido poss√≠vel para economizar tokens***
  Use uma linguagem formal mas descontra√≠da.
  Pule uma linha entre o resumo e o tom de voz.
  D√™ prefer√™ncia ao pre√ßo parcelado.
  
  Nome do cliente: ${nome}
  
  Ao final, sempre fa√ßa perguntas utilizando esse documento como base:
  TOM DE VOZ:
  ${JSON.stringify(tomDeVozVertexData, null, 2)}`
      },
      {
        role: "user",
        content: `Modelo: ${modelo.nome}\nFrase de impacto: ${modelo.fraseImpacto}\nDescri√ß√£o curta: ${modelo.descricaoCurta}`
      }
    ];
  
    let resumo = "";
    try {
      const resposta = await openai.chat.completions.create({
        model: "gpt-4",
        messages: prompt,
        temperature: 0.99,
        max_tokens: 150
      });
  
      resumo = resposta.choices?.[0]?.message?.content?.trim() || "";
    } catch (err) {
      console.error("‚ùå Erro ao gerar resumo com GPT:", err);
      resumo = `üì± *${modelo.nome}*\n${modelo.fraseImpacto}\nüí∞ R$ ${modelo.preco.toFixed(2)}\n\nEm breve te explico mais!`;
    }
  
    // üíæ Salva modelo no hist√≥rico
    await appendToConversation(sender, {
      tipo: "modelo_sugerido_json",
      conteudo: modelo,
      timestamp: new Date().toISOString()
    });

    // üíæ Salva tamb√©m como modelo confirmado (para refer√™ncia futura)
await appendToConversation(sender, {
  tipo: "modelo_confirmado",
  conteudo: modelo.nome,
  timestamp: new Date().toISOString()
});

  
    // üìπ Envia o v√≠deo com resumo na legenda
    if (modelo.videoURL) {
      return await sendBotMessage(sender, {
        videoUrl: modelo.videoURL,
        caption: resumo
      });
    }
  
    // üìÑ Se n√£o tiver v√≠deo, envia s√≥ o texto
    return await sendBotMessage(sender, resumo);
  },  
  responderDuvida: async (sender, args, extras) => {
    await setUserStage(sender, "agente_de_demonstra√ß√£o_detalhada_boleto");

    const { msgContent, quotedMessage } = extras;

    const entrada = await sanitizarEntradaComQuoted(sender, msgContent, quotedMessage);

    const { modelos, nomeUsuario,  modelosConfirmados, conversaCompleta } = await prepararContextoDeModelosRecentes(sender);

    if (modelos.length === 0) {
      return await sendBotMessage(sender, "‚ö†Ô∏è Ainda n√£o te mostrei nenhum modelo pra comparar. Quer ver algumas op√ß√µes?");
    }

    // üéØ üí• Aqui entra a corre√ß√£o: se veio nomeModelo no argumento, foque nesse modelo
    const modeloFocado = args?.nomeModelo
      ? modelos.find(m => m.nome.toLowerCase() === args.nomeModelo.toLowerCase())
      : null;

    let descricaoModelos = "";

    if (modeloFocado) {
      descricaoModelos = `
  ‚û°Ô∏è *${modeloFocado.nome}*
  üí¨ Descri√ß√£o: ${modeloFocado.descricaoCurta}
  üß† Subt√≠tulo: ${modeloFocado.subTitulo}
  üí° Frase impacto: ${modeloFocado.fraseImpacto}
  üíµ Pre√ßo: R$ ${modeloFocado.preco.toFixed(2)}
  üí≥ Parcelado: ${modeloFocado.precoParcelado}
  üñºÔ∏è Imagem: ${modeloFocado.imagemURL}
  `;
    } else {
      descricaoModelos = modelos.map(m => `
  ‚û°Ô∏è *${m.nome}*
  üí¨ Descri√ß√£o: ${m.descricaoCurta}
  üß† Subt√≠tulo: ${m.subTitulo}
  üí° Frase impacto: ${m.fraseImpacto}
  üíµ Pre√ßo: R$ ${m.preco.toFixed(2)}
  üí≥ Parcelado: ${m.precoParcelado}
  üñºÔ∏è Imagem: ${m.imagemURL}
  `).join("\n");
    }


    const contexto = `
    Voc√™ √© Anna, especialista da Vertex Store.
    
    Siga exatamente as diretrizes abaixo para responder qualquer cliente:
    
    TOM DE VOZ:
    ${JSON.stringify(tomDeVozVertex, null, 2)}
    
    OBJE√á√ïES COMUNS:
    ${JSON.stringify(obje√ß√µesVertexBoleto, null, 2).slice(0, 3000)}

       OBJE√á√ïES SOBRE PAYJOY:
    ${JSON.stringify(informacoesPayjoy).slice(0, 3500)}
    
    GATILHOS EMOCIONAIS:
    ${JSON.stringify(gatilhosEmocionaisVertex, null, 2)}
    `;

    // üß† Prompt formatado para a IA
    const prompt = `
  ## OBJETIVO
  Guiar o cliente at√© escolher um smartphone da lista apresentada e fechar a venda,
  sempre valorizando experi√™ncia, suporte humanizado e diferencial da loja.
  esteja sempre preparado para responder duvidas de obje√ß√µes que n√£o necessariamente ligados ao modelo em si, utlize a documenta√ß√£o para respoder essa obje√ß√µes e seja criativo
  *** SEMPRE AO FALAR DE PRE√áOS DEIXE BEM CLARO QUE ESSE VALORES S√ÉO ESTIMATIVAS E QUE PODEM FLUTUAR DE ACORDO COM A DISPONIBILIDADE DA PAY JOY ***
  ## TOM_DE_VOZ
  - Sauda√ß√£o acolhedora por√©m direta.
  - Use vocativo informal respeitoso (ex.: ‚ÄúPerfeito, ${nomeUsuario}!‚Äù).
  - Emojis: üíú obrigat√≥rio + 1 contextual; use üî• para descontos.
  - At√© 250 caracteres por bloco; quebre linhas por assunto.
  - Pontua√ß√£o dupla (‚Äú!!‚Äù, ‚Äú‚Ä¶‚Äù ) permitida.

  ## GATILHOS_EMOCIONAIS
  - Priorize Seguran√ßa ‚ûú Rapidez ‚ûú Transfer√™ncia de dados na hora.
  - Explore ‚ÄúGarantia emp√°tica‚Äù, ‚ÄúTelefone reserva‚Äù, ‚ÄúLoja f√≠sica confi√°vel‚Äù.
  - Conecte benef√≠cios √† vida di√°ria (produtividade, mem√≥rias, status).

  ## OBJE√á√ïES & COMPARATIVOS
  - Se cliente comparar pre√ßo online ‚Üí explique valor agregado (lista de diferenciais).
  - Descontos: no boleto n√£o descontos
  - Parcelamento padr√£o apenas em 18√ó somente parcelamos em 18x; .
  - Use analogias para comparar servi√ßos (ex.: ‚Äúcomprar s√≥ pre√ßo √© como‚Ä¶‚Äù).

   ## OBJE√á√ïES DE DUVIDAS SOBRE BOLETO(OBJE√á√ïES SOBRE PAYJOY:)

  ## REGRAS_DE_ESTILO
  - Nunca comece com sauda√ß√£o completa; a conversa j√° est√° em andamento.
  - Seja conciso e humanizado; m√°ximo 3 blocos (‚Äúemo√ß√£o‚Äù, ‚Äúbenef√≠cio‚Äù, ‚Äúcall-to-action‚Äù).
  - Sempre feche perguntando algo que avance (ex.: ‚ÄúFecho em 10√ó pra voc√™?‚Äù).

  üìú Hist√≥rico da conversa:
        ${conversaCompleta}
      
      üß† √öltima mensagem do cliente:
      "${entrada}"
      
      üì± Modelos apresentados:
      ${modelos.map(m => `‚û°Ô∏è *${m.nome}*\nüìù ${m.descricaoCurta}\nüíµ Pre√ßo: R$ ${m.preco.toFixed(2)}`).join("\n")}
      
      Nome do cliente: ${nomeUsuario}
      
      ‚úÖ Modelos confirmados anteriormente pelo cliente:
      ${modelosConfirmados.length > 0
        ? modelosConfirmados.map(m => `‚úîÔ∏è *${m}*`).join("\n")
        : "Nenhum ainda foi confirmado."}
      
      üß† √öltimo modelo confirmado:
      ${modelosConfirmados[modelosConfirmados.length - 1] || "nenhum"}
  `;

    const respostaIA = await openai.chat.completions.create({
      model: "gpt-4o",
      messages: [
        { role: "system", content: contexto },
        { role: "user", content: prompt }
      ],
      temperature: 1.0,
      max_tokens: 200
    });

    const respostaFinal = respostaIA.choices[0]?.message?.content?.trim();

    if (!respostaFinal) {
      return await sendBotMessage(sender, "üìå Estou verificando... Pode repetir a d√∫vida de forma diferente?");
    }

    return await sendBotMessage(sender, respostaFinal);
  },

}

 
module.exports = {
  agenteDeDemonstracaoDetalhadaBoleto,
  handlers
  
};

