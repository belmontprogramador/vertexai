const axios = require("axios");
const { pipeline } = require("@xenova/transformers");
const { storeSentMessage } = require("./messageService");
const { PrismaClient } = require("@prisma/client");
const OpenAI = require("openai");

const prisma = new PrismaClient();

const INSTANCE_ID = process.env.INSTANCE_ID;
const API_URL = `https://api.w-api.app/v1/message/send-text?instanceId=${INSTANCE_ID}`;
const TOKEN = process.env.TOKEN;
const BOT_PHONE_NUMBER = process.env.BOT_PHONE_NUMBER || "5522981413041";
const OPENAI_API_KEY = process.env.OPENAI_API_KEY;

let embedder = null;
const openai = new OpenAI({ apiKey: OPENAI_API_KEY });

// üîÑ Carrega o modelo de embeddings (se ainda n√£o foi carregado)
const loadModel = async () => {
  if (!embedder) {
    console.log("üîÑ Carregando modelo de embeddings...");
    embedder = await pipeline("feature-extraction", "Xenova/all-MiniLM-L6-v2");
    console.log("‚úÖ Modelo carregado com sucesso!");
  }
};

// üìå Gera embeddings da mensagem
const generateEmbedding = async (text) => {
  try {
    await loadModel();
    const embedding = await embedder(text, { pooling: "mean", normalize: true });
    return Array.from(embedding.data);
  } catch (error) {
    console.error("‚ùå Erro ao gerar embedding:", error);
    return [];
  }
};

// üîç Busca as √∫ltimas 50 mensagens enviadas PELO USU√ÅRIO
const getUserMessageHistory = async (senderId) => {
  try {
    const messages = await prisma.userMessage.findMany({
      where: { senderId },
      orderBy: { createdAt: "desc" },
      take: 50,
      select: { conversation: true },
    });

    const history = messages.map((msg) => msg.conversation).filter(Boolean);
    
    // üöÄ LOG: √öltimas mensagens do usu√°rio
    console.log(`üìú √öltimas 50 mensagens enviadas pelo usu√°rio (${senderId}):`, history);

    return history;
  } catch (error) {
    console.error("‚ùå Erro ao buscar hist√≥rico do usu√°rio:", error);
    return [];
  }
};

// üîç Busca as √∫ltimas 50 mensagens enviadas PARA o usu√°rio (humanos + bot)
const getConversationHistoryWithUser = async (senderId) => {
  try {
    const messages = await prisma.sentMessage.findMany({
      where: { recipientId: senderId },
      orderBy: { createdAt: "desc" },
      take: 50,
      select: { content: true, isAI: true },
    });

    const conversation = messages.map((msg) => `(${msg.isAI ? "Bot" : "Humano"}) ${msg.content}`).filter(Boolean);
    
    // üöÄ LOG: √öltimas mensagens enviadas para o usu√°rio
    console.log(`üí¨ √öltimas 50 mensagens enviadas PARA o usu√°rio (${senderId}):`, conversation);

    return conversation;
  } catch (error) {
    console.error("‚ùå Erro ao buscar hist√≥rico de mensagens para o usu√°rio:", error);
    return [];
  }
};

// üîÆ Gera uma resposta usando o ChatGPT com base no contexto, hist√≥rico do usu√°rio e mensagens recebidas
const generateChatGPTResponse = async (senderId, context, userMessage) => {
  try {
    const userHistory = await getUserMessageHistory(senderId);
    const conversationHistory = await getConversationHistoryWithUser(senderId);

    let contextText = context.map((msg, i) => `Mensagem ${i + 1}: "${msg.content}"`).join("\n");
    let historyText = userHistory.map((msg, i) => `Usu√°rio ${i + 1}: "${msg}"`).join("\n");
    let conversationText = conversationHistory.map((msg, i) => `Mensagem ${i + 1}: "${msg}"`).join("\n");

    const prompt = `
Voc√™ √© um assistente virtual da Vertex Store. Responda ao usu√°rio com base nas informa√ß√µes que j√° foram trocadas anteriormente.

### CONTEXTO DE MENSAGENS SEMELHANTES:
${contextText || "Nenhuma mensagem relevante encontrada no hist√≥rico."}

### HIST√ìRICO DAS √öLTIMAS MENSAGENS DO USU√ÅRIO:
${historyText || "Nenhum hist√≥rico de conversa dispon√≠vel."}

### HIST√ìRICO DAS √öLTIMAS RESPOSTAS PARA O USU√ÅRIO:
${conversationText || "Nenhum hist√≥rico de conversa dispon√≠vel."}

### MENSAGEM DO USU√ÅRIO:
"${userMessage}"

### SUA RESPOSTA:
Forne√ßa uma resposta clara e objetiva, utilizando o contexto e o hist√≥rico do usu√°rio sempre que poss√≠vel para manter a coer√™ncia da conversa.
    `;

    const response = await openai.chat.completions.create({
      model: "gpt-4",
      messages: [{ role: "system", content: prompt }],
      temperature: 0.7,
    });

    return response.choices[0].message.content;
  } catch (error) {
    console.error("‚ùå Erro ao gerar resposta com ChatGPT:", error);
    return "Desculpe, n√£o consegui entender sua pergunta.";
  }
};

// üöÄ Processa e envia resposta baseada no hist√≥rico, contexto e ChatGPT
const processAndSendMessage = async (senderId, content) => {
  if (content.toLowerCase().startsWith("kisuco")) {
    console.log(`üöÄ Mensagem detectada com palavra-chave "kisuco". Processando resposta contextual...`);

    const userEmbedding = await generateEmbedding(content);
    const chatResponse = await generateChatGPTResponse(senderId, [], content);

    await sendBotMessage(senderId, chatResponse);
  }
};

// üì§ Envia mensagem e armazena no banco com isAI: true
const sendBotMessage = async (recipientId, content) => {
  try {
    const response = await axios.post(API_URL, {
      phone: recipientId,
      message: content,
      delayMessage: 5,
    }, {
      headers: {
        Authorization: `Bearer ${TOKEN}`,
        "Content-Type": "application/json",
      },
    });

    const messageId = response.data.messageId || `BOT_MSG_${Date.now()}`;

    console.log(`‚úÖ Mensagem enviada pelo bot para ${recipientId}: ${content} (ID: ${messageId})`);

    const embedding = await generateEmbedding(content);

    await storeSentMessage({
      messageId,
      senderId: BOT_PHONE_NUMBER,
      verifiedBizName: "Vertex Store",
      recipientId,
      content,
      embedding,
      isAI: true,
    });

    return response.data;
  } catch (error) {
    console.error("‚ùå Erro ao enviar mensagem do bot:", error.response?.data || error.message);
    throw error;
  }
};

module.exports = { processAndSendMessage, sendBotMessage };
